{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
      "metadata": {
        "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832"
      },
      "source": [
        "## Assignment 3: $k$ Nearest Neighbor\n",
        "\n",
        "`! git clone https://github.com/ds3001f25/knn_assignment.git`\n",
        "\n",
        "**Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9212c0",
      "metadata": {
        "id": "5d9212c0"
      },
      "source": [
        "**Q1.**\n",
        "1. What is the difference between regression and classification?\n",
        "2. What is a confusion table? What does it help us understand about a model's performance?\n",
        "3. What does the SSE quantify about a particular model?\n",
        "4. What are overfitting and underfitting?\n",
        "5. Why does splitting the data into training and testing sets, and choosing $k$ by evaluating accuracy or SSE on the test set, improve model performance?\n",
        "6. With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 Responses:**\n",
        "1. Regression is when a numeric outcome is predicted while classification predicts a categorical outcome. Although both are types of machine learning tasks the difference appears in the type of output they predict\n",
        "Ex:\n",
        "*   Numeric: What are the sales likely to be?\n",
        "*   Categorical: What class of vehicle is it likely to be?\n",
        "2. A confusion table is a cross-tablulation of predicted and actual values. Confusion tables help us to understand classification predictions, it is a metric for whether a model has done well or poorly.\n",
        "3. SSE = Residual sum of squares. The sum of squares is a statistical measure of variability. Indicating the dispersion of data points around the mean. It is the difference between the observed and the predicted values, and tells us the total unexplained variation in the data failed to be captured by the model. It also can help show model accuracy and the goodness data patterns.\n",
        "4. Overfitting occurs when your model is too complex to reliably explain the phenomenon you are interested in. Underfitting occurs when your model is too simple to reliably explain the phenomenon you are interested in.\n",
        "5. Splitting the data up into training or test sets improves the models performance when involved with unseen data. The model learns patterns from the training set data then the test set evaluates how well the trained model performes on the unseen data.\n",
        "Choosing k through accuracy or SSE also improves performance because it generalizes the data well instead of just memorizing what was trained. It also prevents overfitting and keeps the measure realistic to how it would perform in the real world.\n",
        "6. Class label Prediction: The model picks one class it thinks is the most likely.\n",
        "Ex: \"Dog\"\n",
        "Strength: Simple andn clear, very good when only one final class matters.\n",
        "Weakness: Not any other information given. Model might not always be correct if the class amounts are close.\n",
        "Probability: Probability for each class\n",
        "Ex:\n",
        "Dog: 70%, Cat: 20%, Pig: 10%\n",
        "Strength: Levels of confidence for each class.\n",
        "Weakness: More technical data, not just one clear cut answer. Needs a well tuned, accurate, working model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFI2I_HlRwBM"
      },
      "id": "MFI2I_HlRwBM"
    },
    {
      "cell_type": "markdown",
      "id": "194455fa",
      "metadata": {
        "id": "194455fa"
      },
      "source": [
        "**Q2.** This question is a case study for $k$ nearest neighbor regression, using the `USA_cars_datasets.csv` data.\n",
        "\n",
        "The target variable `y` is `price` and the features are `year` and `mileage`.\n",
        "\n",
        "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
        "2. Maxmin normalize `year` and `mileage`.\n",
        "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
        "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
        "5. Determine the optimal $k$ for these data.\n",
        "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "739292a2",
      "metadata": {
        "id": "739292a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.1\n",
        "cd = pd.read_csv('USA_cars_datasets.csv')\n",
        "#cd.head()\n",
        "\n",
        "cd1 = cd[['price', 'year', 'mileage']]\n",
        "print(cd1.head())\n",
        "print(cd1.shape, '\\n')\n",
        "\n",
        "print(cd1['mileage'].isna().any())\n",
        "print(cd1['price'].isna().any())\n",
        "print(cd1['year'].isna().any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj4FvjyFCYof",
        "outputId": "b7340a4f-7fde-4c46-d05b-3c1c53e97d70"
      },
      "id": "vj4FvjyFCYof",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   price  year  mileage\n",
            "0   6300  2008   274117\n",
            "1   2899  2011   190552\n",
            "2   5350  2018    39590\n",
            "3  25000  2014    64146\n",
            "4  27700  2018     6654\n",
            "(2499, 3) \n",
            "\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2\n",
        "def maxmin(z):\n",
        "  z = (z-z.min())/(z.max()-z.min())\n",
        "  return z\n",
        "\n",
        "cd['year'] = maxmin(cd['year'])\n",
        "cd['mileage'] = maxmin(cd['mileage'])"
      ],
      "metadata": {
        "id": "6B9fBnGGGEZi"
      },
      "id": "6B9fBnGGGEZi",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.3\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cd1, test_size=0.2)\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UrpGDC7NBnB",
        "outputId": "98db23af-59ea-49f0-97ce-008a9911a3be"
      },
      "id": "1UrpGDC7NBnB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1999, 3)\n",
            "Test shape: (500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4\n",
        "def knn_reg(x_hat,gdf,K):\n",
        "  squared_differences = (gdf[['year', 'mileage']].values - x_hat)**2\n",
        "  distances = np.sqrt(squared_differences.sum(axis=1))\n",
        "  neighbors = np.argsort(distances)[:K].tolist()\n",
        "  y_star = gdf['price'].iloc[neighbors].tolist()\n",
        "  y_hat = np.mean(y_star)\n",
        "  return {'y_hat': y_hat, 'y_star':y_star, 'neighbors':neighbors}"
      ],
      "metadata": {
        "id": "WusS-uEEKTC8"
      },
      "id": "WusS-uEEKTC8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d193de6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8d193de6"
      },
      "source": [
        "**Q3.** This is a case study on $k$ nearest neighbor regression and imputation, using the `airbnb_hw.csv` data.\n",
        "\n",
        "There are 30,478 observations, but only 22,155 ratings. We're going to build a kNN regressor to impute missing values. This is a common task, and illustrates one way you can use kNN in the future even when you have more advanced models available.\n",
        "\n",
        "1. Load the `airbnb_hw.csv` data with Pandas. We're only going to use `Review Scores Rating`, `Price`, and `Beds`, so use `.loc` to reduce the dataframe to those variables.\n",
        "2. Set use `.isnull()` to select the subset of the dataframe with missing review values. Set those aside in a different dataframe. We'll make predictions about them later.\n",
        "3. Use `df = df.dropna(axis = 0, how = 'any')` to eliminate any observations with missing values/NA's from the dataframe.\n",
        "4. For the complete cases, create a $k$-NN model that uses the variables `Price` and `Beds` to predict `Review Scores Rating`. How do you choose $k$? (Hint: Train/test split, iterate over reasonable values of $k$ and find a value that minimizes SSE on the test split using predictions from the training set.)\n",
        "5. Predict the missing ratings.\n",
        "6. Do a kernel density plot of the training ratings and the predicted missing ratings. Do they look similar or not? Explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b362aa0d",
      "metadata": {
        "id": "b362aa0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}